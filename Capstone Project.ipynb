{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# US Immigration Data - warehouse\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "In this project we gather data for a sample data warehouse for analysing US immigration data in a simple star schema. The main aim is to provide analysts the possibility to answer business questions using simple tools. The final data is stored in parquet files in local store.. \n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Importing all required python packages\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "A spark local session is initiated and utilised for data exploration, cleaning and implementation of the ETL process. Parquet files are exported locally after the data analytics and wrangling is done to create dim and fact tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Spark session\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.repositories\", \"https://repos.spark-packages.org/\").\\\n",
    "config(\"spark.jars.packages\", \"saurfang:spark-sas7bdat:2.0.0-s_2.11\").\\\n",
    "enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "The scope of this project to integrate I94 immigration data with other data sources including world temperature data, US demographic data and airport codes to create a simple star schema data warehouse for analytical purposes. \n",
    "\n",
    "Should this task be utilised my a large number of people and loaded regularly, all the necessary steps outlined here should be implemented in a separate .py file.\n",
    "\n",
    "In this first step data is read into Spark and also to a pandas dataframe for analysis\n",
    "\n",
    "#### Datasets\n",
    "1. I94 Immigration data [link](https://www.trade.gov/national-travel-and-tourism-office)\n",
    "2. World Temperature Data [link](https://www.kaggle.com/datasets/berkeleyearth/climate-change-earth-surface-temperature-data)\n",
    "3. US City Demographic Data [link](https://public.opendatasoft.com/explore/dataset/us-cities-demographics/export/)\n",
    "4. Airport Codes table [link](https://datahub.io/core/airport-codes#data)\n",
    "5. Reference tables for abbreviations of country, city and state and lookup tables for abbreviations\n",
    "\n",
    "#### 1. I94 Immigration Data \n",
    "This data comes from the US National Tourism and Trade Office."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read in I94 Immigration Data\n",
    "i94_path = '../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat'\n",
    "# Pandas df for analytical purposes\n",
    "df_i94 = pd.read_sas(i94_path, 'sas7bdat', encoding=\"ISO-8859-1\")\n",
    "# Spark df as staging\n",
    "spark_i94 = spark.read.format('com.github.saurfang.sas.spark').load(i94_path)\n",
    "spark_i94.createOrReplaceTempView(\"stage_immigration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>i94port</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>depdate</th>\n",
       "      <th>...</th>\n",
       "      <th>entdepu</th>\n",
       "      <th>matflag</th>\n",
       "      <th>biryear</th>\n",
       "      <th>dtaddto</th>\n",
       "      <th>gender</th>\n",
       "      <th>insnum</th>\n",
       "      <th>airline</th>\n",
       "      <th>admnum</th>\n",
       "      <th>fltno</th>\n",
       "      <th>visatype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>692.0</td>\n",
       "      <td>692.0</td>\n",
       "      <td>XXX</td>\n",
       "      <td>20573.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>U</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1979.0</td>\n",
       "      <td>10282016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.897628e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>276.0</td>\n",
       "      <td>ATL</td>\n",
       "      <td>20551.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>AL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>D/S</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.736796e+09</td>\n",
       "      <td>00296</td>\n",
       "      <td>F1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>WAS</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MI</td>\n",
       "      <td>20691.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1961.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OS</td>\n",
       "      <td>6.666432e+08</td>\n",
       "      <td>93</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1988.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AA</td>\n",
       "      <td>9.246846e+10</td>\n",
       "      <td>00199</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AA</td>\n",
       "      <td>9.246846e+10</td>\n",
       "      <td>00199</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   cicid   i94yr  i94mon  i94cit  i94res i94port  arrdate  i94mode i94addr  \\\n",
       "0    6.0  2016.0     4.0   692.0   692.0     XXX  20573.0      NaN     NaN   \n",
       "1    7.0  2016.0     4.0   254.0   276.0     ATL  20551.0      1.0      AL   \n",
       "2   15.0  2016.0     4.0   101.0   101.0     WAS  20545.0      1.0      MI   \n",
       "3   16.0  2016.0     4.0   101.0   101.0     NYC  20545.0      1.0      MA   \n",
       "4   17.0  2016.0     4.0   101.0   101.0     NYC  20545.0      1.0      MA   \n",
       "\n",
       "   depdate   ...     entdepu  matflag  biryear   dtaddto gender insnum  \\\n",
       "0      NaN   ...           U      NaN   1979.0  10282016    NaN    NaN   \n",
       "1      NaN   ...           Y      NaN   1991.0       D/S      M    NaN   \n",
       "2  20691.0   ...         NaN        M   1961.0  09302016      M    NaN   \n",
       "3  20567.0   ...         NaN        M   1988.0  09302016    NaN    NaN   \n",
       "4  20567.0   ...         NaN        M   2012.0  09302016    NaN    NaN   \n",
       "\n",
       "  airline        admnum  fltno visatype  \n",
       "0     NaN  1.897628e+09    NaN       B2  \n",
       "1     NaN  3.736796e+09  00296       F1  \n",
       "2      OS  6.666432e+08     93       B2  \n",
       "3      AA  9.246846e+10  00199       B2  \n",
       "4      AA  9.246846e+10  00199       B2  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_i94.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "|cicid| i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear| dtaddto|gender|insnum|airline|        admnum|fltno|visatype|\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "|  6.0|2016.0|   4.0| 692.0| 692.0|    XXX|20573.0|   null|   null|   null|  37.0|    2.0|  1.0|    null|    null| null|      T|   null|      U|   null| 1979.0|10282016|  null|  null|   null| 1.897628485E9| null|      B2|\n",
      "|  7.0|2016.0|   4.0| 254.0| 276.0|    ATL|20551.0|    1.0|     AL|   null|  25.0|    3.0|  1.0|20130811|     SEO| null|      G|   null|      Y|   null| 1991.0|     D/S|     M|  null|   null|  3.73679633E9|00296|      F1|\n",
      "| 15.0|2016.0|   4.0| 101.0| 101.0|    WAS|20545.0|    1.0|     MI|20691.0|  55.0|    2.0|  1.0|20160401|    null| null|      T|      O|   null|      M| 1961.0|09302016|     M|  null|     OS|  6.66643185E8|   93|      B2|\n",
      "| 16.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     MA|20567.0|  28.0|    2.0|  1.0|20160401|    null| null|      O|      O|   null|      M| 1988.0|09302016|  null|  null|     AA|9.246846133E10|00199|      B2|\n",
      "| 17.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     MA|20567.0|   4.0|    2.0|  1.0|20160401|    null| null|      O|      O|   null|      M| 2012.0|09302016|  null|  null|     AA|9.246846313E10|00199|      B2|\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_i94.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 2. World Temperature Data\n",
    "This dataset came from Kaggle. The data is by date, country and state as this matched the requirements the most"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "WT_path = './GlobalLandTemperaturesByState.csv'\n",
    "# Pandas df for analytical purposes\n",
    "df_WT = pd.read_csv(WT_path)\n",
    "# Spark df as staging\n",
    "spark_WT = spark.read.format(\"csv\").option(\"header\", \"true\").load(WT_path)\n",
    "spark_WT.createOrReplaceTempView(\"stage_temperature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>AverageTemperature</th>\n",
       "      <th>AverageTemperatureUncertainty</th>\n",
       "      <th>State</th>\n",
       "      <th>Country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1855-05-01</td>\n",
       "      <td>25.544</td>\n",
       "      <td>1.171</td>\n",
       "      <td>Acre</td>\n",
       "      <td>Brazil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1855-06-01</td>\n",
       "      <td>24.228</td>\n",
       "      <td>1.103</td>\n",
       "      <td>Acre</td>\n",
       "      <td>Brazil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1855-07-01</td>\n",
       "      <td>24.371</td>\n",
       "      <td>1.044</td>\n",
       "      <td>Acre</td>\n",
       "      <td>Brazil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1855-08-01</td>\n",
       "      <td>25.427</td>\n",
       "      <td>1.073</td>\n",
       "      <td>Acre</td>\n",
       "      <td>Brazil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1855-09-01</td>\n",
       "      <td>25.675</td>\n",
       "      <td>1.014</td>\n",
       "      <td>Acre</td>\n",
       "      <td>Brazil</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           dt  AverageTemperature  AverageTemperatureUncertainty State Country\n",
       "0  1855-05-01              25.544                          1.171  Acre  Brazil\n",
       "1  1855-06-01              24.228                          1.103  Acre  Brazil\n",
       "2  1855-07-01              24.371                          1.044  Acre  Brazil\n",
       "3  1855-08-01              25.427                          1.073  Acre  Brazil\n",
       "4  1855-09-01              25.675                          1.014  Acre  Brazil"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_WT.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+-----------------------------+-----+-------+\n",
      "|        dt|AverageTemperature|AverageTemperatureUncertainty|State|Country|\n",
      "+----------+------------------+-----------------------------+-----+-------+\n",
      "|1855-05-01|            25.544|                        1.171| Acre| Brazil|\n",
      "|1855-06-01|            24.228|                        1.103| Acre| Brazil|\n",
      "|1855-07-01|            24.371|                        1.044| Acre| Brazil|\n",
      "|1855-08-01|            25.427|                        1.073| Acre| Brazil|\n",
      "|1855-09-01|            25.675|                        1.014| Acre| Brazil|\n",
      "+----------+------------------+-----------------------------+-----+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_WT.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 3. U.S. City Demographic Data\n",
    "This data comes from OpenSoft."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "DD_path = './us-cities-demographics.csv'\n",
    "# Pandas df for analytical purposes\n",
    "df_DD = pd.read_csv(DD_path, delimiter=\";\")\n",
    "# Spark df as staging\n",
    "spark_DD = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"delimiter\", \";\").load(DD_path)\n",
    "spark_DD.createOrReplaceTempView(\"stage_demography\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Median Age</th>\n",
       "      <th>Male Population</th>\n",
       "      <th>Female Population</th>\n",
       "      <th>Total Population</th>\n",
       "      <th>Number of Veterans</th>\n",
       "      <th>Foreign-born</th>\n",
       "      <th>Average Household Size</th>\n",
       "      <th>State Code</th>\n",
       "      <th>Race</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Silver Spring</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>33.8</td>\n",
       "      <td>40601.0</td>\n",
       "      <td>41862.0</td>\n",
       "      <td>82463</td>\n",
       "      <td>1562.0</td>\n",
       "      <td>30908.0</td>\n",
       "      <td>2.60</td>\n",
       "      <td>MD</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>25924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Quincy</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>41.0</td>\n",
       "      <td>44129.0</td>\n",
       "      <td>49500.0</td>\n",
       "      <td>93629</td>\n",
       "      <td>4147.0</td>\n",
       "      <td>32935.0</td>\n",
       "      <td>2.39</td>\n",
       "      <td>MA</td>\n",
       "      <td>White</td>\n",
       "      <td>58723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hoover</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>38.5</td>\n",
       "      <td>38040.0</td>\n",
       "      <td>46799.0</td>\n",
       "      <td>84839</td>\n",
       "      <td>4819.0</td>\n",
       "      <td>8229.0</td>\n",
       "      <td>2.58</td>\n",
       "      <td>AL</td>\n",
       "      <td>Asian</td>\n",
       "      <td>4759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rancho Cucamonga</td>\n",
       "      <td>California</td>\n",
       "      <td>34.5</td>\n",
       "      <td>88127.0</td>\n",
       "      <td>87105.0</td>\n",
       "      <td>175232</td>\n",
       "      <td>5821.0</td>\n",
       "      <td>33878.0</td>\n",
       "      <td>3.18</td>\n",
       "      <td>CA</td>\n",
       "      <td>Black or African-American</td>\n",
       "      <td>24437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Newark</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>34.6</td>\n",
       "      <td>138040.0</td>\n",
       "      <td>143873.0</td>\n",
       "      <td>281913</td>\n",
       "      <td>5829.0</td>\n",
       "      <td>86253.0</td>\n",
       "      <td>2.73</td>\n",
       "      <td>NJ</td>\n",
       "      <td>White</td>\n",
       "      <td>76402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               City          State  Median Age  Male Population  \\\n",
       "0     Silver Spring       Maryland        33.8          40601.0   \n",
       "1            Quincy  Massachusetts        41.0          44129.0   \n",
       "2            Hoover        Alabama        38.5          38040.0   \n",
       "3  Rancho Cucamonga     California        34.5          88127.0   \n",
       "4            Newark     New Jersey        34.6         138040.0   \n",
       "\n",
       "   Female Population  Total Population  Number of Veterans  Foreign-born  \\\n",
       "0            41862.0             82463              1562.0       30908.0   \n",
       "1            49500.0             93629              4147.0       32935.0   \n",
       "2            46799.0             84839              4819.0        8229.0   \n",
       "3            87105.0            175232              5821.0       33878.0   \n",
       "4           143873.0            281913              5829.0       86253.0   \n",
       "\n",
       "   Average Household Size State Code                       Race  Count  \n",
       "0                    2.60         MD         Hispanic or Latino  25924  \n",
       "1                    2.39         MA                      White  58723  \n",
       "2                    2.58         AL                      Asian   4759  \n",
       "3                    3.18         CA  Black or African-American  24437  \n",
       "4                    2.73         NJ                      White  76402  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_DD.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+-----+\n",
      "|            City|        State|Median Age|Male Population|Female Population|Total Population|Number of Veterans|Foreign-born|Average Household Size|State Code|                Race|Count|\n",
      "+----------------+-------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+-----+\n",
      "|   Silver Spring|     Maryland|      33.8|          40601|            41862|           82463|              1562|       30908|                   2.6|        MD|  Hispanic or Latino|25924|\n",
      "|          Quincy|Massachusetts|      41.0|          44129|            49500|           93629|              4147|       32935|                  2.39|        MA|               White|58723|\n",
      "|          Hoover|      Alabama|      38.5|          38040|            46799|           84839|              4819|        8229|                  2.58|        AL|               Asian| 4759|\n",
      "|Rancho Cucamonga|   California|      34.5|          88127|            87105|          175232|              5821|       33878|                  3.18|        CA|Black or African-...|24437|\n",
      "|          Newark|   New Jersey|      34.6|         138040|           143873|          281913|              5829|       86253|                  2.73|        NJ|               White|76402|\n",
      "+----------------+-------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_DD.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4. Airport Code Table\n",
    "This is a simple table of airport codes and corresponding cities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "AC_path = './airport-codes_csv.csv'\n",
    "# Pandas df for analytical purposes\n",
    "df_AC = pd.read_csv(AC_path)\n",
    "# Spark df as staging\n",
    "spark_AC = spark.read.format(\"csv\").option(\"header\", \"true\").load(AC_path)\n",
    "spark_AC.createOrReplaceTempView(\"stage_airport\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ident</th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>elevation_ft</th>\n",
       "      <th>continent</th>\n",
       "      <th>iso_country</th>\n",
       "      <th>iso_region</th>\n",
       "      <th>municipality</th>\n",
       "      <th>gps_code</th>\n",
       "      <th>iata_code</th>\n",
       "      <th>local_code</th>\n",
       "      <th>coordinates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00A</td>\n",
       "      <td>heliport</td>\n",
       "      <td>Total Rf Heliport</td>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-PA</td>\n",
       "      <td>Bensalem</td>\n",
       "      <td>00A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00A</td>\n",
       "      <td>-74.93360137939453, 40.07080078125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00AA</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Aero B Ranch Airport</td>\n",
       "      <td>3435.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-KS</td>\n",
       "      <td>Leoti</td>\n",
       "      <td>00AA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AA</td>\n",
       "      <td>-101.473911, 38.704022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00AK</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Lowell Field</td>\n",
       "      <td>450.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AK</td>\n",
       "      <td>Anchor Point</td>\n",
       "      <td>00AK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AK</td>\n",
       "      <td>-151.695999146, 59.94919968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00AL</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Epps Airpark</td>\n",
       "      <td>820.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AL</td>\n",
       "      <td>Harvest</td>\n",
       "      <td>00AL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AL</td>\n",
       "      <td>-86.77030181884766, 34.86479949951172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00AR</td>\n",
       "      <td>closed</td>\n",
       "      <td>Newport Hospital &amp; Clinic Heliport</td>\n",
       "      <td>237.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AR</td>\n",
       "      <td>Newport</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-91.254898, 35.6087</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ident           type                                name  elevation_ft  \\\n",
       "0   00A       heliport                   Total Rf Heliport          11.0   \n",
       "1  00AA  small_airport                Aero B Ranch Airport        3435.0   \n",
       "2  00AK  small_airport                        Lowell Field         450.0   \n",
       "3  00AL  small_airport                        Epps Airpark         820.0   \n",
       "4  00AR         closed  Newport Hospital & Clinic Heliport         237.0   \n",
       "\n",
       "  continent iso_country iso_region  municipality gps_code iata_code  \\\n",
       "0       NaN          US      US-PA      Bensalem      00A       NaN   \n",
       "1       NaN          US      US-KS         Leoti     00AA       NaN   \n",
       "2       NaN          US      US-AK  Anchor Point     00AK       NaN   \n",
       "3       NaN          US      US-AL       Harvest     00AL       NaN   \n",
       "4       NaN          US      US-AR       Newport      NaN       NaN   \n",
       "\n",
       "  local_code                            coordinates  \n",
       "0        00A     -74.93360137939453, 40.07080078125  \n",
       "1       00AA                 -101.473911, 38.704022  \n",
       "2       00AK            -151.695999146, 59.94919968  \n",
       "3       00AL  -86.77030181884766, 34.86479949951172  \n",
       "4        NaN                    -91.254898, 35.6087  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_AC.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "|ident|         type|                name|elevation_ft|continent|iso_country|iso_region|municipality|gps_code|iata_code|local_code|         coordinates|\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "|  00A|     heliport|   Total Rf Heliport|          11|       NA|         US|     US-PA|    Bensalem|     00A|     null|       00A|-74.9336013793945...|\n",
      "| 00AA|small_airport|Aero B Ranch Airport|        3435|       NA|         US|     US-KS|       Leoti|    00AA|     null|      00AA|-101.473911, 38.7...|\n",
      "| 00AK|small_airport|        Lowell Field|         450|       NA|         US|     US-AK|Anchor Point|    00AK|     null|      00AK|-151.695999146, 5...|\n",
      "| 00AL|small_airport|        Epps Airpark|         820|       NA|         US|     US-AL|     Harvest|    00AL|     null|      00AL|-86.7703018188476...|\n",
      "| 00AR|       closed|Newport Hospital ...|         237|       NA|         US|     US-AR|     Newport|    null|     null|      null| -91.254898, 35.6087|\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_AC.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 5. Reference Tables\n",
    "Lookup tables for further use in datamodel. These are partly provided or derived from the data or created by me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# read description file\n",
    "RT_path = './I94_SAS_Labels_Descriptions.SAS'\n",
    "with open(RT_path) as fl:\n",
    "    SAS_file = fl.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# country reference file\n",
    "country_reference = dict()\n",
    "for countries in SAS_file[10:298]:\n",
    "    values = countries.split('=')\n",
    "    code, country = values[0].strip(), values[1].strip().strip(\"'\")\n",
    "    country_reference[code] = country\n",
    "\n",
    "spark_RT_country = spark.createDataFrame(country_reference.items(), ['code', 'country'])\n",
    "\n",
    "spark_RT_country.createOrReplaceTempView(\"ref_country\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+\n",
      "|code|    country|\n",
      "+----+-----------+\n",
      "| 236|AFGHANISTAN|\n",
      "| 101|    ALBANIA|\n",
      "| 316|    ALGERIA|\n",
      "| 102|    ANDORRA|\n",
      "| 324|     ANGOLA|\n",
      "+----+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_RT_country.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# city reference file\n",
    "city_reference = dict()\n",
    "for cities in SAS_file[303:962]:\n",
    "    values = cities.split('=')\n",
    "    code, city = values[0].strip(\"\\t\").strip().strip(\"'\"), values[1].strip(\"\\t\").strip().strip(\"''\")\n",
    "    city_reference[code] = city\n",
    "\n",
    "spark_RT_city = spark.createDataFrame(city_reference.items(), ['code', 'city'])\n",
    "\n",
    "spark_RT_city.createOrReplaceTempView(\"ref_city\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+\n",
      "|code|                city|\n",
      "+----+--------------------+\n",
      "| ANC|ANCHORAGE, AK    ...|\n",
      "| BAR|BAKER AAF - BAKER...|\n",
      "| DAC|DALTONS CACHE, AK...|\n",
      "| PIZ|DEW STATION PT LA...|\n",
      "| DTH|DUTCH HARBOR, AK ...|\n",
      "+----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_RT_city.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# state reference file\n",
    "state_reference = dict()\n",
    "for states in SAS_file[982:1036]:\n",
    "    values = states.split('=')\n",
    "    code, state = values[0].strip(\"\\t\").strip(\"'\"), values[1].strip().strip(\"'\")\n",
    "    state_reference[code] = state\n",
    "\n",
    "spark_RT_state = spark.createDataFrame(state_reference.items(), ['code', 'state'])\n",
    "\n",
    "spark_RT_state.createOrReplaceTempView(\"ref_state\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+\n",
      "|code|     state|\n",
      "+----+----------+\n",
      "|  AK|    ALASKA|\n",
      "|  AZ|   ARIZONA|\n",
      "|  AR|  ARKANSAS|\n",
      "|  CA|CALIFORNIA|\n",
      "|  CO|  COLORADO|\n",
      "+----+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_RT_state.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "iata_reference = spark.sql(\n",
    "\"\"\"\n",
    "    SELECT DISTINCT \n",
    "        a.iata_code\n",
    "    FROM stage_airport AS a\n",
    "    WHERE a.iata_code IS NOT NULL\n",
    "        AND a.iso_country LIKE 'US'\n",
    "        AND a.type IN ('large_airport', 'medium_airport', 'small_airport')\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|iata_code|\n",
      "+---------+\n",
      "|      BZT|\n",
      "|      BGM|\n",
      "|      CNU|\n",
      "|      CRS|\n",
      "|      KEB|\n",
      "+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iata_reference.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "iata_reference.createOrReplaceTempView(\"tmp_iata_airline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "iata_reference = spark.sql(\n",
    "\"\"\"\n",
    "    SELECT DISTINCT \n",
    "        a.i94port\n",
    "    FROM stage_immigration AS a\n",
    "    WHERE a.i94port IN (\n",
    "        SELECT b.iata_code\n",
    "        FROM tmp_iata_airline AS b)\n",
    "\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|i94port|\n",
      "+-------+\n",
      "|    FMY|\n",
      "|    BGM|\n",
      "|    DNS|\n",
      "|    FOK|\n",
      "|    HVR|\n",
      "+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iata_reference.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "iata_reference.createOrReplaceTempView(\"tmp_iata_immigration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "As the last step the initial data loads to stage tables are checked - the simple row counts are gathered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# stage & reference tables check\n",
    "rows =spark.sql(\n",
    "\"\"\"\n",
    "    SELECT\n",
    "        'stage_immigration' AS tbl\n",
    "        , count(*) AS rowcount\n",
    "    FROM stage_immigration AS a \n",
    "    \n",
    "    UNION\n",
    "    \n",
    "    SELECT\n",
    "        'stage_temperature' AS tbl\n",
    "        , count(*) AS rowcount\n",
    "    FROM stage_temperature AS b \n",
    "    \n",
    "    UNION\n",
    "    \n",
    "    SELECT\n",
    "        'stage_airport' AS tbl\n",
    "        , count(*) AS rowcount\n",
    "    FROM stage_airport AS b \n",
    "    \n",
    "    UNION\n",
    "    \n",
    "    SELECT\n",
    "        'stage_demography' AS tbl\n",
    "        , count(*) AS rowcount\n",
    "    FROM stage_demography AS b \n",
    "    \n",
    "    UNION\n",
    "    \n",
    "    SELECT\n",
    "        'ref_country' AS tbl\n",
    "        , count(*) AS rowcount\n",
    "    FROM ref_country AS b \n",
    "    \n",
    "    UNION\n",
    "    \n",
    "    SELECT\n",
    "        'ref_city' AS tbl\n",
    "        , count(*) AS rowcount\n",
    "    FROM ref_city AS b \n",
    "    \n",
    "    UNION\n",
    "    \n",
    "    SELECT\n",
    "        'ref_state' AS tbl\n",
    "        , count(*) AS rowcount\n",
    "    FROM ref_state AS b \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------+\n",
      "|              tbl|rowcount|\n",
      "+-----------------+--------+\n",
      "|stage_immigration| 3096313|\n",
      "|         ref_city|     659|\n",
      "|stage_temperature|  645675|\n",
      "|    stage_airport|   55075|\n",
      "|      ref_country|     288|\n",
      "| stage_demography|    2891|\n",
      "|        ref_state|      54|\n",
      "+-----------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rows.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# write stage tables to parquet files\n",
    "#fact_immigration.write.mode(\"overwrite\").partitionBy('state_code')\\\n",
    "#                    .parquet(path=output_data + 'fact_immigration')\n",
    "#spark_i94.write.mode(\"overwrite\").parquet(\"stage_immigration\")\n",
    "#spark_WT.write.mode(\"overwrite\").parquet(\"stage_temperature\")\n",
    "#spark_DD.write.mode(\"overwrite\").parquet(\"stage_demography\")\n",
    "#spark_AC.write.mode(\"overwrite\").parquet(\"stage_airport\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### 2.1 Explore the Data \n",
    "Data exploration is carried out here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 2.1.1 Explore i94 Data\n",
    "Explore i94 data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "How many rows are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "tmp =spark.sql(\n",
    "\"\"\"\n",
    "    SELECT count(*) AS rownum\n",
    "    FROM stage_immigration AS a \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "| rownum|\n",
      "+-------+\n",
      "|3096313|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "What dates does this data run for?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "tmp =spark.sql(\n",
    "\"\"\"\n",
    "    SELECT min(date_add('1960-01-01', a.arrdate)) AS min_arrdate, max(date_add('1960-01-01', a.arrdate)) AS max_arrdate\n",
    "    FROM stage_immigration AS a \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "|min_arrdate|max_arrdate|\n",
      "+-----------+-----------+\n",
      "| 2016-04-01| 2016-04-30|\n",
      "+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "How many rows have invalid state codes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# number of rows with invalid state codes\n",
    "tmp = spark.sql(\n",
    "\"\"\"\n",
    "    SELECT COUNT(*) FROM (\n",
    "    SELECT *\n",
    "    FROM stage_immigration AS a\n",
    "    WHERE a.i94addr NOT IN (\n",
    "        SELECT DISTINCT\n",
    "            b.`State Code`\n",
    "        FROM stage_demography AS b\n",
    "        )\n",
    "        ) s\n",
    "\"\"\"       \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|  123652|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "what are these? is it safe to delete?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# invalid rows\n",
    "tmp = spark.sql(\n",
    "\"\"\"\n",
    "\n",
    "    SELECT a.i94addr\n",
    "        , count(*)\n",
    "    FROM stage_immigration AS a\n",
    "    WHERE a.i94addr NOT IN (\n",
    "        SELECT DISTINCT\n",
    "            b.`State Code`\n",
    "        FROM stage_demography AS b\n",
    "        )\n",
    "    GROUP BY a.i94addr\n",
    "    ORDER BY 2 DESC\n",
    "    \n",
    "\n",
    "\"\"\"       \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+\n",
      "|i94addr|count(1)|\n",
      "+-------+--------+\n",
      "|     GU|   94107|\n",
      "|     US|    7421|\n",
      "|     MP|    4505|\n",
      "|     VQ|    4503|\n",
      "|     UN|    1522|\n",
      "+-------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "are there rows with null departure date?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "tmp =spark.sql(\n",
    "\"\"\"\n",
    "    SELECT count(*) AS rownum\n",
    "    FROM stage_immigration AS a \n",
    "    WHERE a.arrdate IS NULL\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|rownum|\n",
      "+------+\n",
      "|     0|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "are there rows where dept date is later than arrival date?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "tmp =spark.sql(\n",
    "\"\"\"\n",
    "    SELECT count(*) AS rownum\n",
    "    FROM stage_immigration AS a \n",
    "    WHERE a.arrdate > a.depdate\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|rownum|\n",
      "+------+\n",
      "|   375|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 2.1.2 Explore World Temperature Data\n",
    "Explore World Temperature Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+-----------------------------+-----+-------+\n",
      "|        dt|AverageTemperature|AverageTemperatureUncertainty|State|Country|\n",
      "+----------+------------------+-----------------------------+-----+-------+\n",
      "|1855-05-01|            25.544|                        1.171| Acre| Brazil|\n",
      "|1855-06-01|            24.228|                        1.103| Acre| Brazil|\n",
      "|1855-07-01|            24.371|                        1.044| Acre| Brazil|\n",
      "|1855-08-01|            25.427|                        1.073| Acre| Brazil|\n",
      "|1855-09-01|            25.675|                        1.014| Acre| Brazil|\n",
      "+----------+------------------+-----------------------------+-----+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_WT.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "number of relevant data (size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "tmp =spark.sql(\n",
    "\"\"\"\n",
    "    SELECT count(*) AS rowcount\n",
    "    FROM stage_temperature AS a \n",
    "    WHERE Country LIKE 'United States'\n",
    "        AND dt > '2000-01-01'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|rowcount|\n",
      "+--------+\n",
      "|    8364|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Check the minimum and maximum dates that the dataset runs for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "tmp =spark.sql(\n",
    "\"\"\"\n",
    "    SELECT min(a.dt), max(a.dt)\n",
    "    FROM stage_temperature AS a \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|   min(dt)|   max(dt)|\n",
      "+----------+----------+\n",
      "|1743-11-01|2013-09-01|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "The WT data is only up until Q4 2013. As this is a demo database, in order to match the i94 data, I have selected 2012 and shifted it to 2016. Later a new database source for WT data should be researched and utilised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Does this data seem feasible? Does it have outliers? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "tmp =spark.sql(\n",
    "\"\"\"\n",
    "    SELECT min(a.AverageTemperature), max(a.AverageTemperature)\n",
    "    FROM stage_temperature AS a \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-----------------------+\n",
      "|min(AverageTemperature)|max(AverageTemperature)|\n",
      "+-----------------------+-----------------------+\n",
      "|   -0.00099999999999...|                  9.999|\n",
      "+-----------------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Looks like no outliers. Also mark, that the data is for months, so the keys should be created for that. \n",
    "\n",
    "The data will be aggregated using GROUP BY so no need to check for duplicates - it will be handled automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 2.1.4 Explore Airline Data\n",
    "Explore Airline Data\n",
    "\n",
    "Sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "tmp = spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT *\n",
    "    FROM stage_airport AS a\n",
    "    WHERE iso_country LIKE 'US'\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "|ident|         type|                name|elevation_ft|continent|iso_country|iso_region|municipality|gps_code|iata_code|local_code|         coordinates|\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "|  00A|     heliport|   Total Rf Heliport|          11|       NA|         US|     US-PA|    Bensalem|     00A|     null|       00A|-74.9336013793945...|\n",
      "| 00AA|small_airport|Aero B Ranch Airport|        3435|       NA|         US|     US-KS|       Leoti|    00AA|     null|      00AA|-101.473911, 38.7...|\n",
      "| 00AK|small_airport|        Lowell Field|         450|       NA|         US|     US-AK|Anchor Point|    00AK|     null|      00AK|-151.695999146, 5...|\n",
      "| 00AL|small_airport|        Epps Airpark|         820|       NA|         US|     US-AL|     Harvest|    00AL|     null|      00AL|-86.7703018188476...|\n",
      "| 00AR|       closed|Newport Hospital ...|         237|       NA|         US|     US-AR|     Newport|    null|     null|      null| -91.254898, 35.6087|\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Number of relevant rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "tmp = spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT count(*)\n",
    "    FROM stage_airport AS a\n",
    "    WHERE iso_country LIKE 'US'\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|   22757|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Types of airports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "tmp = spark.sql(\n",
    "\"\"\"\n",
    "    SELECT DISTINCT \n",
    "        a.type\n",
    "    FROM stage_airport AS a\n",
    "    WHERE a.iata_code IS NOT NULL\n",
    "        AND a.iso_country LIKE 'US'\n",
    "\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|          type|\n",
      "+--------------+\n",
      "| large_airport|\n",
      "| seaplane_base|\n",
      "|      heliport|\n",
      "|        closed|\n",
      "|medium_airport|\n",
      "| small_airport|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Missing airline codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# how many records have no valid airline code?\n",
    "tmp = spark.sql(\n",
    "\"\"\"\n",
    "    SELECT count(*)\n",
    "    FROM stage_immigration b\n",
    "    WHERE b.i94port NOT IN (\n",
    "        SELECT DISTINCT \n",
    "            c.i94port\n",
    "        FROM tmp_iata_immigration AS c\n",
    "        )\n",
    "\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "| 2059350|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# what are these?\n",
    "tmp = spark.sql(\n",
    "\"\"\"\n",
    "    SELECT b.i94port, \n",
    "        count(*)\n",
    "    FROM stage_immigration b\n",
    "    WHERE b.i94port NOT IN (\n",
    "        SELECT DISTINCT \n",
    "            c.i94port\n",
    "        FROM tmp_iata_immigration AS c\n",
    "        )\n",
    "    GROUP BY b.i94port\n",
    "    ORDER BY 2 DESC\n",
    "\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+\n",
      "|i94port|count(1)|\n",
      "+-------+--------+\n",
      "|    NYC|  485916|\n",
      "|    LOS|  310163|\n",
      "|    SFR|  152586|\n",
      "|    ORL|  149195|\n",
      "|    HHW|  142720|\n",
      "|    CHI|  130564|\n",
      "|    FTL|   95977|\n",
      "|    LVG|   89280|\n",
      "|    AGA|   80919|\n",
      "|    WAS|   74835|\n",
      "+-------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 2.1.4 Explore Demography Data\n",
    "Explore Demography Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "number of relevant rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "tmp = spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT count(*)\n",
    "    FROM stage_demography AS a\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|    2891|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "are there missing data in total population?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "tmp = spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT count(*) AS rownum\n",
    "    FROM stage_demography AS a\n",
    "        WHERE a.`Total Population` IS NULL\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|rownum|\n",
      "+------+\n",
      "|     0|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 2.2 Cleaning Steps\n",
    "Document steps necessary to clean the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 2.2.1 Cleaning i94 Immigration Data\n",
    "Cleaning steps for i94 data\n",
    "\n",
    "The i94 data arrival and departure dates should be converted to dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# dates are created\n",
    "spark_i94 = spark_i94.withColumn('arrdate', f.expr(\"date_add('1960-01-01', arrdate)\"))\n",
    "spark_i94 = spark_i94.withColumn('depdate', f.expr(\"date_add('1960-01-01', depdate)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark_i94 = spark_i94.withColumn('reference_date', f.expr(\"date_add(arrdate, -day(arrdate)+1)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+------+------+------+-------+----------+-------+-------+----------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+--------------+\n",
      "|cicid| i94yr|i94mon|i94cit|i94res|i94port|   arrdate|i94mode|i94addr|   depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear| dtaddto|gender|insnum|airline|        admnum|fltno|visatype|reference_date|\n",
      "+-----+------+------+------+------+-------+----------+-------+-------+----------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+--------------+\n",
      "|  6.0|2016.0|   4.0| 692.0| 692.0|    XXX|2016-04-29|   null|   null|      null|  37.0|    2.0|  1.0|    null|    null| null|      T|   null|      U|   null| 1979.0|10282016|  null|  null|   null| 1.897628485E9| null|      B2|    2016-04-01|\n",
      "|  7.0|2016.0|   4.0| 254.0| 276.0|    ATL|2016-04-07|    1.0|     AL|      null|  25.0|    3.0|  1.0|20130811|     SEO| null|      G|   null|      Y|   null| 1991.0|     D/S|     M|  null|   null|  3.73679633E9|00296|      F1|    2016-04-01|\n",
      "| 15.0|2016.0|   4.0| 101.0| 101.0|    WAS|2016-04-01|    1.0|     MI|2016-08-25|  55.0|    2.0|  1.0|20160401|    null| null|      T|      O|   null|      M| 1961.0|09302016|     M|  null|     OS|  6.66643185E8|   93|      B2|    2016-04-01|\n",
      "| 16.0|2016.0|   4.0| 101.0| 101.0|    NYC|2016-04-01|    1.0|     MA|2016-04-23|  28.0|    2.0|  1.0|20160401|    null| null|      O|      O|   null|      M| 1988.0|09302016|  null|  null|     AA|9.246846133E10|00199|      B2|    2016-04-01|\n",
      "| 17.0|2016.0|   4.0| 101.0| 101.0|    NYC|2016-04-01|    1.0|     MA|2016-04-23|   4.0|    2.0|  1.0|20160401|    null| null|      O|      O|   null|      M| 2012.0|09302016|  null|  null|     AA|9.246846313E10|00199|      B2|    2016-04-01|\n",
      "+-----+------+------+------+------+-------+----------+-------+-------+----------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_i94.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark_i94.createOrReplaceTempView(\"stage_immigration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# remove data with invalid state codes\n",
    "spark_i94 = spark.sql(\n",
    "\"\"\"\n",
    "    SELECT *\n",
    "    FROM stage_immigration AS a\n",
    "    WHERE a.i94addr IN (\n",
    "        SELECT DISTINCT\n",
    "            b.`State Code`\n",
    "        FROM stage_demography AS b\n",
    "        )\n",
    "\"\"\"       \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark_i94.createOrReplaceTempView(\"stage_immigration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# remove data with invalid arrival and departure dates\n",
    "spark_i94 = spark.sql(\n",
    "\"\"\"\n",
    "    SELECT *\n",
    "    FROM stage_immigration AS a\n",
    "    WHERE a.arrdate < a.depdate\n",
    "\"\"\"       \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark_i94.createOrReplaceTempView(\"stage_immigration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "tmp = spark.sql(\n",
    "\"\"\"\n",
    "    SELECT count(*)\n",
    "    FROM stage_immigration\n",
    "\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "| 2703145|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tmp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 2.2.2 Cleaning World Temperature Data\n",
    "Cleaning steps for World Temperature Data\n",
    "\n",
    "The World Temperature Data should be filtered for the reference date and for United States. Also as demonstration purposes the dates are shifted to match i94 data. This should not be done in a live database though!\n",
    "\n",
    "Any duplicates should be handled in dim_table creation - aggregation is done there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# filter for US and for a given year\n",
    "spark_WT =spark.sql(\n",
    "\"\"\"\n",
    "    SELECT\n",
    "        add_months(b.dt, 48) as dt\n",
    "        , b.AverageTemperature\n",
    "        , b.AverageTemperatureUncertainty\n",
    "        , b.State\n",
    "        , b.Country\n",
    "    FROM (\n",
    "        SELECT a.dt\n",
    "            , a.AverageTemperature\n",
    "            , a.AverageTemperatureUncertainty\n",
    "            , a.State\n",
    "            , a.Country\n",
    "        FROM stage_temperature AS a \n",
    "        WHERE a.Country LIKE 'United States'\n",
    "            AND year(a.dt) = 2012\n",
    "        ) b\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+-----------------------------+-------+-------------+\n",
      "|        dt|AverageTemperature|AverageTemperatureUncertainty|  State|      Country|\n",
      "+----------+------------------+-----------------------------+-------+-------------+\n",
      "|2016-01-01|             10.29|                        0.303|Alabama|United States|\n",
      "|2016-02-01|            11.405|                        0.201|Alabama|United States|\n",
      "|2016-03-01|            18.401|                        0.124|Alabama|United States|\n",
      "|2016-04-01|            18.675|                        0.265|Alabama|United States|\n",
      "|2016-05-01|            23.512|                        0.225|Alabama|United States|\n",
      "+----------+------------------+-----------------------------+-------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_WT.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark_WT.createOrReplaceTempView(\"stage_temperature\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "States need to be matched with that of i94 as the spelling is different. A lookup table is loaded to the workspace for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# match states with i94\n",
    "distinct_states =spark.sql(\n",
    "\"\"\"\n",
    "    SELECT DISTINCT\n",
    "        a.State\n",
    "    FROM stage_temperature AS a \n",
    "    WHERE a.Country LIKE 'United States'\n",
    "    ORDER BY 1\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|     State|\n",
      "+----------+\n",
      "|   Alabama|\n",
      "|    Alaska|\n",
      "|   Arizona|\n",
      "|  Arkansas|\n",
      "|California|\n",
      "+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "distinct_states.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "WT_lookup_path = './states_i94_weather.csv'\n",
    "# Pandas df for analytical purposes\n",
    "df_lookup_WT = pd.read_csv(WT_lookup_path)\n",
    "# Spark df as staging\n",
    "spark_lookup_WT = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"delimiter\", \";\").load(WT_lookup_path)\n",
    "spark_lookup_WT.createOrReplaceTempView(\"ref_temperature_states\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+-------------+\n",
      "|code| state_i94|state_weather|\n",
      "+----+----------+-------------+\n",
      "|  AL|   ALABAMA|      Alabama|\n",
      "|  AK|    ALASKA|       Alaska|\n",
      "|  AZ|   ARIZONA|      Arizona|\n",
      "|  AR|  ARKANSAS|     Arkansas|\n",
      "|  CA|CALIFORNIA|   California|\n",
      "+----+----------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_lookup_WT.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark_WT =spark.sql(\n",
    "\"\"\"\n",
    "    SELECT a.dt\n",
    "        , b.state_i94 as state\n",
    "        , b.code as state_code\n",
    "        , a.Country\n",
    "        , a.AverageTemperature\n",
    "        , a.AverageTemperatureUncertainty\n",
    "    FROM stage_temperature AS a\n",
    "    LEFT JOIN ref_temperature_states AS b\n",
    "        ON a.State = b.state_weather\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+----------+-------------+------------------+-----------------------------+\n",
      "|        dt|  state|state_code|      Country|AverageTemperature|AverageTemperatureUncertainty|\n",
      "+----------+-------+----------+-------------+------------------+-----------------------------+\n",
      "|2016-01-01|ALABAMA|        AL|United States|             10.29|                        0.303|\n",
      "|2016-02-01|ALABAMA|        AL|United States|            11.405|                        0.201|\n",
      "|2016-03-01|ALABAMA|        AL|United States|            18.401|                        0.124|\n",
      "|2016-04-01|ALABAMA|        AL|United States|            18.675|                        0.265|\n",
      "|2016-05-01|ALABAMA|        AL|United States|            23.512|                        0.225|\n",
      "+----------+-------+----------+-------------+------------------+-----------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_WT.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark_WT.createOrReplaceTempView(\"stage_temperature\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 2.2.4 Cleaning Airport Data\n",
    "Cleaning steps for Airport Data\n",
    "\n",
    "Only need US Airports that are large or medium and that have a match in immigration data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark_AC = spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT *\n",
    "    FROM stage_airport AS a\n",
    "    WHERE a.type IN ('large_airport', 'medium_airport', 'small_airport')\n",
    "        AND a.iso_country LIKE 'US'\n",
    "        AND a.iata_code IS NOT NULL\n",
    "   \n",
    "    \"\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+--------------------+------------+---------+-----------+----------+-------------+--------+---------+----------+--------------------+\n",
      "|ident|         type|                name|elevation_ft|continent|iso_country|iso_region| municipality|gps_code|iata_code|local_code|         coordinates|\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+-------------+--------+---------+----------+--------------------+\n",
      "| 07FA|small_airport|Ocean Reef Club A...|           8|       NA|         US|     US-FL|    Key Largo|    07FA|      OCA|      07FA|-80.274803161621,...|\n",
      "|  0AK|small_airport|Pilot Station Air...|         305|       NA|         US|     US-AK|Pilot Station|    null|      PQS|       0AK|-162.899994, 61.9...|\n",
      "| 0CO2|small_airport|Crested Butte Air...|        8980|       NA|         US|     US-CO|Crested Butte|    0CO2|      CSE|      0CO2|-106.928341, 38.8...|\n",
      "| 0TE7|small_airport|   LBJ Ranch Airport|        1515|       NA|         US|     US-TX| Johnson City|    0TE7|      JCY|      0TE7|-98.6224975585999...|\n",
      "| 13MA|small_airport|Metropolitan Airport|         418|       NA|         US|     US-MA|       Palmer|    13MA|      PMX|      13MA|-72.3114013671999...|\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+-------------+--------+---------+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_AC.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark_AC.createOrReplaceTempView(\"stage_airport\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 2.2.5 Cleaning Demography Data\n",
    "Cleaning steps for Demography Dataset\n",
    "\n",
    "No steps needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "The selected data model for business analysis is the star schema. Star schema is the most suitable analytical data model for analysing a single business process - in this case the immigration to the US. This methodology is developed as per Kimball methodology.\n",
    "\n",
    "What is the business process? What is the smallest granularity? What are the factors that impact upon the business process itself?\n",
    "\n",
    "A single Fact table is created with the immigration event as granularity.\n",
    "\n",
    "Multiple Dimension tables are created that allow business analysts and professionals to analyse the process of immigration. Dimension tables are for filtering and slicing and dicing of events.\n",
    "\n",
    "```\n",
    "â”œâ”€â”€ FACT\n",
    "â”‚   â””â”€â”€ immigration\n",
    "â””â”€â”€ DIMENSIONS\n",
    "    â”œâ”€â”€ immigrant\n",
    "    â”œâ”€â”€ airline\n",
    "    â”œâ”€â”€ demography\n",
    "    â”œâ”€â”€ temperature\n",
    "    â””â”€â”€ airport\n",
    "\n",
    "```\n",
    "The schema of the final data modell is displayed in the project markdown document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 3.2 Mapping Out Data Pipelines\n",
    "The steps for the data pipeline is mapped as follows:\n",
    "\n",
    "* Load the data into Spark staging tables\n",
    "* Clean data using Spark\n",
    "* Create Dimension tables using Spark SQL and create unique IDs for Dimension tables\n",
    "* Create Fact table with uniqe IDs and map Dimension table IDs as foreign keys.\n",
    "* Write data into parquet files\n",
    "* Perform data quality checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Dimension tables and Fact table is created using Spark. Data is exported as parquet files for analytical purposes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.1.1 Dim Immigrant\n",
    "The Dim Immigrant table is created and loaded as a spark temp view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# select required dim data for immigrant\n",
    "dim_immigrant = spark.sql(\n",
    "\"\"\"\n",
    "    SELECT DISTINCT\n",
    "        int(a.cicid) AS cic_id\n",
    "        , b.country AS citizen_country\n",
    "        , c.country AS residence_country\n",
    "        , int(a.biryear) AS birth_year\n",
    "        , a.gender AS gender\n",
    "        , int(a.insnum) AS ins_num\n",
    "    FROM stage_immigration AS a \n",
    "    LEFT JOIN ref_country b ON\n",
    "        a.i94cit = b.code\n",
    "    LEFT JOIN ref_country c ON\n",
    "        a.i94res = c.code\n",
    "    \n",
    "\"\"\")\n",
    "\n",
    "# add primary key as running integer\n",
    "dim_immigrant = dim_immigrant.withColumn('immigrant_id', f.monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------------+-----------------+----------+------+-------+------------+\n",
      "|cic_id|citizen_country|residence_country|birth_year|gender|ins_num|immigrant_id|\n",
      "+------+---------------+-----------------+----------+------+-------+------------+\n",
      "|304334|       MONGOLIA|         MONGOLIA|      1967|     F|   null|           0|\n",
      "|213969|        ECUADOR|          ECUADOR|      1944|     M|   null|           1|\n",
      "|214420|        ECUADOR|          ECUADOR|      1973|     F|   null|           2|\n",
      "|214688|        ECUADOR|          ECUADOR|      1991|     F|   null|           3|\n",
      "|214811|        ECUADOR|          ECUADOR|      1997|     F|   null|           4|\n",
      "+------+---------------+-----------------+----------+------+-------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_immigrant.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dim_immigrant.createOrReplaceTempView(\"dim_immigrant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cic_id: integer (nullable = true)\n",
      " |-- citizen_country: string (nullable = true)\n",
      " |-- residence_country: string (nullable = true)\n",
      " |-- birth_year: integer (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- ins_num: integer (nullable = true)\n",
      " |-- immigrant_id: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_immigrant.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Saving the data in parquet format\n",
    "dim_immigrant.write.mode(\"overwrite\").parquet(\"dim_immigrant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.1.2 Dim Airline\n",
    "The Dim Airline table is created and loaded as a spark temp view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# select required dim data for airline\n",
    "dim_airline = spark.sql(\n",
    "\"\"\"\n",
    "    SELECT DISTINCT\n",
    "        int(a.cicid) AS cic_id\n",
    "        , a.airline AS airline\n",
    "        , int(a.admnum) AS admin_num\n",
    "        , a.fltno AS flight_number\n",
    "        , a.visatype AS visa_type\n",
    "    FROM stage_immigration AS a \n",
    "\"\"\")\n",
    "\n",
    "# add primary key as running integer\n",
    "dim_airline = dim_airline.withColumn('airline_id', f.monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+----------+-------------+---------+----------+\n",
      "|cic_id|airline| admin_num|flight_number|visa_type|airline_id|\n",
      "+------+-------+----------+-------------+---------+----------+\n",
      "|   118|     LH|2147483647|        00402|       WT|         0|\n",
      "|   375|     AA|2147483647|        00039|       WT|         1|\n",
      "|   693|     AA|2147483647|        00717|       WT|         2|\n",
      "|  1175|     UA|2147483647|        00914|       WT|         3|\n",
      "|  1447|     BA|2147483647|        00175|       WT|         4|\n",
      "+------+-------+----------+-------------+---------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_airline.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dim_airline.createOrReplaceTempView(\"dim_airline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cic_id: integer (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admin_num: integer (nullable = true)\n",
      " |-- flight_number: string (nullable = true)\n",
      " |-- visa_type: string (nullable = true)\n",
      " |-- airline_id: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_airline.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Saving the data in parquet format\n",
    "dim_airline.write.mode(\"overwrite\").parquet(\"dim_airline\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.1.3 Dim Temperature\n",
    "The Dim Temperature table is created and loaded as a spark temp view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# select and aggregate required dim data for temperature\n",
    "dim_temperature = spark.sql(\n",
    "\"\"\"\n",
    "    SELECT \n",
    "        a.dt AS reference_date\n",
    "        , a.state AS state\n",
    "        , a.state_code AS state_code\n",
    "        , a.Country AS country\n",
    "        , round(avg(a.AverageTemperature), 2) AS avg_temp\n",
    "        , round(avg(a.AverageTemperatureUncertainty), 2) AS avg_temp_uncertainty\n",
    "    FROM stage_temperature AS a \n",
    "    GROUP BY a.dt, year(a.dt), month(a.dt), a.state, a.state_code, a.country\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "# add primary key as running integer\n",
    "dim_temperature = dim_temperature.withColumn('temperature_id', f.monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------------+----------+-------------+--------+--------------------+--------------+\n",
      "|reference_date|            state|state_code|      country|avg_temp|avg_temp_uncertainty|temperature_id|\n",
      "+--------------+-----------------+----------+-------------+--------+--------------------+--------------+\n",
      "|    2016-11-01|          ALABAMA|        AL|United States|   11.22|                0.13|             0|\n",
      "|    2016-10-01|DIST. OF COLUMBIA|        DC|United States|   13.61|                0.33|             1|\n",
      "|    2016-10-01|            MAINE|        ME|United States|     9.1|                0.27|    8589934592|\n",
      "|    2016-01-01|       NEW MEXICO|        NM|United States|    3.43|                0.27|    8589934593|\n",
      "|    2016-06-01|        N. DAKOTA|        ND|United States|   19.07|                0.35|    8589934594|\n",
      "+--------------+-----------------+----------+-------------+--------+--------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_temperature.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dim_temperature.createOrReplaceTempView(\"dim_temperature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- reference_date: date (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- state_code: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- avg_temp: double (nullable = true)\n",
      " |-- avg_temp_uncertainty: double (nullable = true)\n",
      " |-- temperature_id: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_temperature.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Saving the data in parquet format\n",
    "dim_temperature.write.mode(\"overwrite\").parquet(\"dim_temperature\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.1.4 Dim Airport\n",
    "The Dim Airport table is created and loaded as a spark temp view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dim_airport = spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT DISTINCT \n",
    "        a.iata_code\n",
    "        , a.name\n",
    "        , a.type\n",
    "        , a.elevation_ft\n",
    "        , float(trim(substr(a.coordinates, locate(',', a.coordinates)+1))) AS lat\n",
    "        , float(trim(left(a.coordinates, locate(',', a.coordinates)-1))) AS lng\n",
    "\n",
    "    FROM stage_airport a\n",
    "\n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "# add primary key as running integer\n",
    "dim_airport = dim_airport.withColumn('airport_id', f.monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+--------------+------------+-------+--------+----------+\n",
      "|iata_code|                name|          type|elevation_ft|    lat|     lng|airport_id|\n",
      "+---------+--------------------+--------------+------------+-------+--------+----------+\n",
      "|      AAF|Apalachicola Regi...| small_airport|          20|29.7275|-85.0275|         0|\n",
      "|      CAE|Columbia Metropol...| large_airport|         236|33.9388|-81.1195|         1|\n",
      "|      EKX|     Addington Field| small_airport|         775| 37.686| -85.925|         2|\n",
      "|      EUF|        Weedon Field| small_airport|         285|31.9513|-85.1289|         3|\n",
      "|      FTK|Godman Army Air F...|medium_airport|         756|37.9071|-85.9721|         4|\n",
      "+---------+--------------------+--------------+------------+-------+--------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_airport.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dim_airport.createOrReplaceTempView(\"dim_airport\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- iata_code: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- elevation_ft: string (nullable = true)\n",
      " |-- lat: float (nullable = true)\n",
      " |-- lng: float (nullable = true)\n",
      " |-- airport_id: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_airport.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Saving the data in parquet format\n",
    "dim_airport.write.mode(\"overwrite\").parquet(\"dim_airport\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.1.5 Dim Demography\n",
    "The Dim Demography table is created and loaded as a spark temp view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dim_demography = spark.sql(\n",
    "\"\"\"\n",
    "    SELECT a.`State Code` AS state_code\n",
    "        , round(avg(a.`Median Age`), 2) AS median_age\n",
    "        , sum(a.`Male Population`) AS  male_population\n",
    "        , sum(a.`Female Population`) AS female_population\n",
    "        , sum(a.`Total Population`) AS total_population\n",
    "        , sum(a.`Number of Veterans`) AS veteran_population\n",
    "        , sum(a.`Foreign-born`) AS foreign_population\n",
    "        , round(avg(a.`Average Household Size`), 2) AS average_household_size\n",
    "    FROM stage_demography AS a\n",
    "    GROUP BY a.`State Code` \n",
    "    ORDER BY 1\n",
    "\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# add primary key as running integer\n",
    "dim_demography = dim_demography.withColumn('demography_id', f.monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+---------------+-----------------+----------------+------------------+------------------+----------------------+-------------+\n",
      "|state_code|median_age|male_population|female_population|total_population|veteran_population|foreign_population|average_household_size|demography_id|\n",
      "+----------+----------+---------------+-----------------+----------------+------------------+------------------+----------------------+-------------+\n",
      "|        AK|      32.2|       764725.0|         728750.0|       1493475.0|          137460.0|          166290.0|                  2.77|            0|\n",
      "|        AL|     36.16|      2448200.0|        2715106.0|       5163306.0|          352896.0|          252541.0|                  2.43|   8589934592|\n",
      "|        AR|     32.74|      1400724.0|        1482165.0|       2882889.0|          154390.0|          307753.0|                  2.53|  17179869184|\n",
      "|        AZ|     35.04|    1.1137275E7|      1.1360435E7|      2.249771E7|         1322525.0|         3411565.0|                  2.77|  25769803776|\n",
      "|        CA|     36.17|    6.1055672E7|      6.2388681E7|    1.23444353E8|         4617022.0|       3.7059662E7|                   3.1|  34359738368|\n",
      "+----------+----------+---------------+-----------------+----------------+------------------+------------------+----------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_demography.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "dim_demography.createOrReplaceTempView(\"dim_demography\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- state_code: string (nullable = true)\n",
      " |-- median_age: double (nullable = true)\n",
      " |-- male_population: double (nullable = true)\n",
      " |-- female_population: double (nullable = true)\n",
      " |-- total_population: double (nullable = true)\n",
      " |-- veteran_population: double (nullable = true)\n",
      " |-- foreign_population: double (nullable = true)\n",
      " |-- average_household_size: double (nullable = true)\n",
      " |-- demography_id: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dim_demography.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Saving the data in parquet format\n",
    "dim_demography.write.mode(\"overwrite\").parquet(\"dim_demography\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.1.7 Fact Immigration\n",
    "The Fact Immigration table is created and loaded as a spark temp view. The necessary columns are selected and renamed, and Foreign Keys are added from Dim tables. A running ID is added as a Primary Key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# data is selected, foreign keys are added and an unique runnig ID is created\n",
    "fact_immigration = spark.sql(\n",
    "\"\"\"\n",
    "    SELECT DISTINCT\n",
    "        int(a.cicid) AS cic_id\n",
    "        , int(a.i94yr) AS year\n",
    "        , int(a.i94mon) AS month\n",
    "        , a.i94port AS city_code\n",
    "        , a.arrdate AS arrive_date\n",
    "        , int(a.i94mode) AS mode\n",
    "        , a.i94addr AS state_code\n",
    "        , a.depdate AS departure_date\n",
    "        , int(a.i94visa) AS visa\n",
    "        , 'United States' AS country\n",
    "        , b.immigrant_id AS immigrant_id\n",
    "        , c.airline_id AS airline_id\n",
    "        , d.temperature_id\n",
    "        , e.demography_id\n",
    "        , f.airport_id\n",
    "    FROM stage_immigration AS a \n",
    "        LEFT JOIN dim_immigrant AS b ON \n",
    "            int(a.cicid) = b.cic_id\n",
    "        LEFT JOIN dim_airline AS c ON\n",
    "            int(a.cicid) = c.cic_id\n",
    "        LEFT JOIN dim_temperature AS d ON\n",
    "            a.i94addr = d.state_code AND\n",
    "            a.reference_date = d.reference_date\n",
    "        LEFT JOIN dim_demography AS e ON\n",
    "            a.i94addr = e.state_code\n",
    "        LEFT JOIN dim_airport AS f ON\n",
    "            a.i94port = f.iata_code\n",
    "\"\"\")\n",
    "\n",
    "# add primary key as running integer\n",
    "fact_immigration = fact_immigration.withColumn('immigration_id', f.monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-----+---------+-----------+----+----------+--------------+----+-------------+-------------+-------------+--------------+-------------+-------------+--------------+\n",
      "| cic_id|year|month|city_code|arrive_date|mode|state_code|departure_date|visa|      country| immigrant_id|   airline_id|temperature_id|demography_id|   airport_id|immigration_id|\n",
      "+-------+----+-----+---------+-----------+----+----------+--------------+----+-------------+-------------+-------------+--------------+-------------+-------------+--------------+\n",
      "|2327332|2016|    4|      DEN| 2016-04-13|   1|        CO|    2016-04-23|   2|United States| 420906804890| 214748369994| 1116691496960|  42949672960| 360777252865|             0|\n",
      "|4293112|2016|    4|      SFB| 2016-04-23|   1|        CO|    2016-05-07|   2|United States|1425929152664| 197568505430| 1116691496960|  42949672960|1657857376261|             1|\n",
      "| 762307|2016|    4|      DEN| 2016-04-04|   1|        CO|    2016-04-08|   2|United States| 146028892202| 472446404250| 1116691496960|  42949672960| 360777252865|             2|\n",
      "|5357662|2016|    4|      ATL| 2016-04-28|   1|        CO|    2016-05-02|   2|United States| 979252548244| 867583405935| 1116691496960|  42949672960| 506806140929|             3|\n",
      "|4716459|2016|    4|      SFR| 2016-04-25|   1|        CO|    2016-04-30|   1|United States| 755914247882|1236950591794| 1116691496960|  42949672960|         null|             4|\n",
      "+-------+----+-----+---------+-----------+----+----------+--------------+----+-------------+-------------+-------------+--------------+-------------+-------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fact_immigration.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "fact_immigration.createOrReplaceTempView(\"fact_immigration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cic_id: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- city_code: string (nullable = true)\n",
      " |-- arrive_date: date (nullable = true)\n",
      " |-- mode: integer (nullable = true)\n",
      " |-- state_code: string (nullable = true)\n",
      " |-- departure_date: date (nullable = true)\n",
      " |-- visa: integer (nullable = true)\n",
      " |-- country: string (nullable = false)\n",
      " |-- immigrant_id: long (nullable = true)\n",
      " |-- airline_id: long (nullable = true)\n",
      " |-- temperature_id: long (nullable = true)\n",
      " |-- demography_id: long (nullable = true)\n",
      " |-- airport_id: long (nullable = true)\n",
      " |-- immigration_id: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fact_immigration.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Saving the data in parquet format\n",
    "fact_immigration.write.mode(\"overwrite\").parquet(\"fact_immigration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    "\n",
    "#### 4.2.1 Simple row count check\n",
    "\n",
    "See if the Fact and Dim tables have data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# dim & fact table check\n",
    "rows =spark.sql(\n",
    "\"\"\"\n",
    "    SELECT\n",
    "        'fact_immigration' AS tbl\n",
    "        , count(*) AS rowcount\n",
    "    FROM fact_immigration AS a \n",
    "    \n",
    "    UNION\n",
    "    \n",
    "    SELECT\n",
    "        'dim_temperature' AS tbl\n",
    "        , count(*) AS rowcount\n",
    "    FROM dim_temperature AS b \n",
    "    \n",
    "    UNION\n",
    "    \n",
    "    SELECT\n",
    "        'dim_airport' AS tbl\n",
    "        , count(*) AS rowcount\n",
    "    FROM dim_airport AS b \n",
    "    \n",
    "    UNION\n",
    "    \n",
    "    SELECT\n",
    "        'dim_demography' AS tbl\n",
    "        , count(*) AS rowcount\n",
    "    FROM dim_demography AS b \n",
    "    \n",
    "    UNION\n",
    "    \n",
    "    SELECT\n",
    "        'dim_airline' AS tbl\n",
    "        , count(*) AS rowcount\n",
    "    FROM dim_airline AS b \n",
    "    \n",
    "    UNION\n",
    "    \n",
    "    SELECT\n",
    "        'dim_immigrant' AS tbl\n",
    "        , count(*) AS rowcount\n",
    "    FROM dim_immigrant AS b \n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------+\n",
      "|             tbl|rowcount|\n",
      "+----------------+--------+\n",
      "|  dim_demography|      49|\n",
      "|   dim_immigrant| 2703145|\n",
      "|     dim_airline| 2703145|\n",
      "| dim_temperature|     612|\n",
      "|fact_immigration| 2703145|\n",
      "|     dim_airport|    1865|\n",
      "+----------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rows.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Data table Primary Key - Foreign Key Check\n",
    "\n",
    "See if Dim table keys match Fact table keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "airport FK check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "tmp = spark.sql(\n",
    "\"\"\"\n",
    "    SELECT count(b.airport_id)\n",
    "    FROM fact_immigration AS a \n",
    "    LEFT JOIN dim_airport AS b \n",
    "        ON a.airport_id = b.airport_id\n",
    "\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|count(airport_id)|\n",
      "+-----------------+\n",
      "|          1149202|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# expected close to row num of fact_immigration (~2.7 million)\n",
    "tmp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "temperature FK check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "tmp = spark.sql(\n",
    "\"\"\"\n",
    "    SELECT count(b.temperature_id)\n",
    "    FROM fact_immigration AS a \n",
    "    LEFT JOIN dim_temperature AS b \n",
    "        ON a.temperature_id = b.temperature_id\n",
    "\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|count(temperature_id)|\n",
      "+---------------------+\n",
      "|              2694191|\n",
      "+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# expected close to row num of fact_immigration (~2.7 million)\n",
    "tmp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "immigrant FK check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "tmp = spark.sql(\n",
    "\"\"\"\n",
    "    SELECT count(b.immigrant_id)\n",
    "    FROM fact_immigration AS a \n",
    "    LEFT JOIN dim_immigrant AS b \n",
    "        ON a.immigrant_id = b.immigrant_id\n",
    "\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|count(immigrant_id)|\n",
      "+-------------------+\n",
      "|            2703145|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# expected close to row num of fact_immigration (~2.7 million)\n",
    "tmp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "airline FK check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "tmp = spark.sql(\n",
    "\"\"\"\n",
    "    SELECT count(b.airline_id)\n",
    "    FROM fact_immigration AS a \n",
    "    LEFT JOIN dim_airline AS b \n",
    "        ON a.airline_id = b.airline_id\n",
    "\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|count(airline_id)|\n",
      "+-----------------+\n",
      "|          2703145|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# expected close to row num of fact_immigration (~2.7 million)\n",
    "tmp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "demography FK check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "tmp = spark.sql(\n",
    "\"\"\"\n",
    "    SELECT count(b.demography_id)\n",
    "    FROM fact_immigration AS a \n",
    "    LEFT JOIN dim_demography AS b \n",
    "        ON a.demography_id = b.demography_id\n",
    "\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|count(demography_id)|\n",
      "+--------------------+\n",
      "|             2703145|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# expected close to row num of fact_immigration (~2.7 million)\n",
    "tmp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "The data dictionary contains the data definitions for the final data model. The following tables contain the data definitions and data types of the resulting data model.\n",
    "\n",
    "#### 4.3.1 Fact table\n",
    "Fact table focuses on the occurrence of an immigration event."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "| Table   name     | Column name    | Description            | Data type           | Primari key |\n",
    "|------------------|----------------|------------------------|---------------------|-------------|\n",
    "| fact_immigration | immigration_id | PK                     | INT (autogenerated) | PRIMARY KEY |\n",
    "| fact_immigration | immigrant_id   | key to immigrant id    | INT                 | FOREIGN KEY |\n",
    "| fact_immigration | airline_id     | key to airline_id      | INT                 | FOREIGN KEY |\n",
    "| fact_immigration | temperature_id | key to temperature_id  | INT                 | FOREIGN KEY |\n",
    "| fact_immigration | demography_id  | key to demography_id   | INT                 | FOREIGN KEY |\n",
    "| fact_immigration | airport_id     | key to demography_id   | INT                 | FOREIGN KEY |\n",
    "| fact_immigration | cic_id         | cicid                  | BIGINT              |             |\n",
    "| fact_immigration | year           | 4 digit year           | INT                 |             |\n",
    "| fact_immigration | month          | numeric month          | INT                 |             |\n",
    "| fact_immigration | city_code      | USA City abbreviation  | CHAR(3)             |             |\n",
    "| fact_immigration | arrive_date    | Arrive date            | TIMESTAMP           |             |\n",
    "| fact_immigration | mode           | Traffic method         | INT                 |             |\n",
    "| fact_immigration | state_code     | USA State abbreviation | CHAR(2)             |             |\n",
    "| fact_immigration | departure_date | Departure date         | TIMESTAMP           |             |\n",
    "| fact_immigration | visa           | Visa category          | INT                 |             |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3.2 Dim tables\n",
    "Dim tables are filter tables and provide reasoning behind the immigration events "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "| Table   name  | Column name       | Description            | Data type           | Primari key |\n",
    "|---------------|-------------------|------------------------|---------------------|-------------|\n",
    "| dim_immigrant | immigrant_id      | PK                     | INT (autogenerated) | PRIMARY KEY |\n",
    "| dim_immigrant | cic_id            | cicid                  | BIGINT              |             |\n",
    "| dim_immigrant | citizen_country   | country of citizenship | VARCHAR             |             |\n",
    "| dim_immigrant | residence_country | country of residence   | VARCHAR             |             |\n",
    "| dim_immigrant | birth_year        | birth year             | INT                 |             |\n",
    "| dim_immigrant | gender            | gender                 | CHAR(1)             |             |\n",
    "| dim_immigrant | ins_num           | INS number             | INT                 |             |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "| Table name  | Column name   | Description              | Data type           | Primari key |\n",
    "|-------------|---------------|--------------------------|---------------------|-------------|\n",
    "| dim_airline | airline_id    | PK                       | INT (autogenerated) | PRIMARY KEY |\n",
    "| dim_airline | cic_id        | cicid                    | BIGINT              |             |\n",
    "| dim_airline | airline       | airline used on arrival  | VARCHAR             |             |\n",
    "| dim_airline | admin_num     | admission number         | BIGINT              |             |\n",
    "| dim_airline | flight_number | flight number on arrival | VARCHAR             |             |\n",
    "| dim_airline | visa_type     | class of legal admission | CHAR(2)             |             |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "| Table   name    | Column name          | Description                 | Data type          | Primari key |\n",
    "|-----------------|----------------------|-----------------------------|--------------------|-------------|\n",
    "| dim_temperature | temperature_id       | PK                          | INT(autogenerated) | PRIMARY KEY |\n",
    "| dim_temperature | reference_date       | date of temperature data    | TIMESTAMP          |             |\n",
    "| dim_temperature | state                | reference city code         | VARCHAR            |             |\n",
    "| dim_temperature | state_code           | state code                  | VARCHAR            |             |\n",
    "| dim_temperature | country              | reference country           | VARCHAR            |             |\n",
    "| dim_temperature | avg_temp             | monthly average temperature | FLOAT              |             |\n",
    "| dim_temperature | avg_temp_uncertainty | temperature uncertainty     | FLOAT              |             |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "| Table   name   | Column name            | Description                  | Data type  | Primari key |\n",
    "|----------------|------------------------|------------------------------|------------|-------------|\n",
    "| dim_demography | demography_id          | INT(autogenerate)            | INT        | PRIMARY KEY |\n",
    "| dim_demography | state_code             | State code                   | VARCHAR(2) |             |\n",
    "| dim_demography | median_age             | Median age                   | FLOAT      |             |\n",
    "| dim_demography | male_population        | Total males                  | INT        |             |\n",
    "| dim_demography | female_population      | Total females                | INT        |             |\n",
    "| dim_demography | total_population       | Total population             | INT        |             |\n",
    "| dim_demography | veteran_population     | Sum of veterans              | INT        |             |\n",
    "| dim_demography | foreign_population     | Sum of foreign born citizens | INT        |             |\n",
    "| dim_demography | average_household_size | Average household size       | FLOAT      |             |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "| Table name  | Column name  | Description                  | Data type | Primari key |\n",
    "|-------------|--------------|------------------------------|-----------|-------------|\n",
    "| dim_airport | airport_id   | INT(autogenerate)            | INT       | PRIMARY KEY |\n",
    "| dim_airport | iata_code    | code abbreviaton for airport | VARCHAR   |             |\n",
    "| dim_airport | name         | name of airport              | VARCHAR   |             |\n",
    "| dim_airport | type         | type of airport              | VARCHAR   |             |\n",
    "| dim_airport | elevation_ft | elevation of airport         | FLOAT     |             |\n",
    "| dim_airport | lat          | latitude of location         | FLOAT     |             |\n",
    "| dim_airport | lng          | longitude of location        | FLOAT     |             |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "#### 5.1 Rationale for choice of tools\n",
    "Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "The project uses Apache Spark engine. Spark is an simple and fast and also scalable analytics engine for large scale data processing.  It has an ability to process and analyse massive ammounts of data using PySpark interface.\n",
    "\n",
    "\n",
    "#### 5.2 Data updates\n",
    "Propose how often the data should be updated and why.\n",
    "The fact table is created on a monthly basis - as such the data set could be updated on a monthly basis. \n",
    "\n",
    "#### 5.3 Scenario analysis\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people.\n",
    "\n",
    "#### 5.3.1 Data volume increase\n",
    "Current implementation is not suitable for handling such a large volumen increase. Should the volume increase, more sophisticated data framework should be implemented. S3 data storage should be used. An Amazon EMR cluster could be utilized  behind Apache Spark. Amazon Redshift distributed datase should be utilized.\n",
    "\n",
    "#### 5.3.2 Data update frequency increase\n",
    "Current ETL process is implemented in Jupyter Notebook. This is not suitable for an automated data pipeline. In such a case the ETL pipeline should be implemented in Apache Airflow using predefined DAGs. Quality checks and timing could be implemented using Airflow.\n",
    "\n",
    "#### 5.3.3 Data user increase\n",
    "Currently no database is created behind the solution as this is designed for analytical purposes for a small scale of users. Should the number of users increase, a distributed database solution should be implemented like Amazon Redshift - this database could handle massive data for a large number of users even if located in different parts of the world."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 5.3 Further Improvements \n",
    "* The weather data only runs until 2013 - this needs to be improved as the fact for immigration is for 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
